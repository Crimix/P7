\chapter{Introduction}\label{ch:intro}
% Needs to be less fluffy.

Ever since the inception of the modern internet, the information giants such as
Google, Yahoo, and Amazon have been aiming for more and more personalization in
their web-services, as the more personally relevant information and products are
to the user, the more likely they are to engage with it 
\citep[p.8]{pariser2011filter}. Over the years, this constant aim to design for
personalization has led to a situation, where web-services are always trying to
present users with the information which is most likely to cause a positive
reaction, while hiding information which could cause a negative
reaction \citep{filterBubbleDef, Personality}. This highlighting and hiding of
information is what activist and author Eli Pariser has coined ``The Filter
Bubble'' \citep[p.9]{pariser2011filter}. He describes the filter bubble as an
invisible filter which users can easily fail to notice, as it is not something
that you actively choose to engage with. This is fundamentally different from
old school media, as the user would expect a certain filter or viewpoint
whenever they turned on a specific news station or news website

\citep[p.10]{pariser2011filter}. While this problem appears to be centered
around the large information giants, Pariser mentions that the problem also
persists on social media like Facebook, where content recommendations are
becoming increasingly personalized to the point where he experienced Facebook no
longer showing him content opposed to his political views
\citep{pariserTedSummary}. And while users might think that the people they
surround themselves with should represent many varied world views, the average
person's Facebook friends will be more likely to have the same opinions, and
share information from the same news sources \citep[p.66]{pariser2011filter}.\nl

If we accept Pariser's idea of the filter bubble, and then consider, that as of
August 2017, upwards of 67\% of adults in the United States report that they use
social media as a source of information \citep{journalism2017}, we can
reasonably see a situation where users are likely to only receive information confirming
their existing viewpoints and beliefs. The existence of this filter
bubble results in the problems which are tackled during this project, namely
how to assist internet users in identifying their filter bubble, and giving them
the necessary tools to break it.

% The internet is widely used to find and gather information about any topics, it
% provides a vast amount of information from many different sources, and it is
% only growing larger. While easy access to information is a definite benefit, the
% enormous amount of information results in each user acquiring a small subset of
% the available knowledge.\\
% 
% Theoretically, this would not be a problem, as users could use the dynamics
% reach of social media to engage with information and news from different
% sources. But in reality, users tend to surround themselves with like-minded
% people, who in turn are prone to share information that correlates with their
% own opinions and biases.\\
% 
% This phenomenon is known as the ``Filter Bubble", which is a term coined by Eli
% Pariser in his 2007 book ``The Filter Bubble" \citep{pariser2011filter}.
% This Bubble refers to the effect of either selectively choosing information
% sources based on personal bias or actual algorithms encoded in the social media
% sites or search engines.
% The aim of which is to present the user with the information which is most
% likely to be regarded positively.
% The existence and enforcement of this bubble can cause problems, in which users
% avoid getting their opinions challenged by preventing them from being exposed to
% information and views that are opposed to the bubble in which they find
% themselves 
% %\citep[p.59-73]{pariser2011filter}\fix{}{So many pages?}.\nl
% 
%  In an interview with Eli Pariser, a
% search on ``Egypt'' shows a case of Google filtering information based on the
% user's preferences. In this example, two users would make a Google search with
% the term ``Egypt'', and end up with two widely different set of results.
% One user got results with information about ongoing conflicts and news; the
% other user saw travel guides and hotel booking information \citep{nusSduSearch}.