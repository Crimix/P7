A crawler, also known as a spider, crawls the web to gather information from the
internet by reading the content of a web-page. Each page usually contains
multiple links to other pages which can also be crawled and indexed. This
recursive behaviour allows a crawler to gather data from internet.

% General webcrawler structure
\section{Crawler design}
During a crawl there are several things to consider, the crawler must be able
to avoid getting stuck fetching an infinite number of pages from the same
domain, as this is only acceptable if you want to crawl that specific site. The
crawler needs to be ''polite'' about how frequently it makes its request, as
too many requests to the same server can crash it, or cause the server to
consider the crawler an attacker and block it. The crawler should respect what
it is and is not permitted to access from a given domain. Additionally, the
crawler should also be able to re-fetch older sites, in order to see if they
have been updated. It should favour fetching pages that are more likely to have
quality content \citep[Ch. 20.1]{manning2008introduction}.\nl

The crawler architecture is designed to be modular, can be seen in
\autoref{BasicWC}. The 'Fetch' module is designed to take a URL from the
frontier and request access to the corresponding site. The site content is
passed to the 'Parse' module, see \autoref{sec:parsing}.
The content is verified to see if it has been crawled before. The URLs are then
filtered according to the domains ``/robots.txt'', which describes what each
crawler is allowed to access. Finally, the duplicate URLs are eliminated, and
the new URLs are added to the frontier.

\figx[0.8]{BasicWC}{The basic crawler architecture \citep[p.
446]{manning2008introduction}.}

% Flesh out more
A more advanced version of a crawler involves managing multiple distributed
crawlers, such that they are able to work together efficiently and not crawl
the same sites multiple times.

\section{Parsing} \label{sec:parsing}
Parsing is a method used to extract information from a document and
index it for later use. The first part of parsing a document consists of
removing the unnecessary information, such as HTML tags, while retaining the
important information, such as links and the body text. Depending on the
document, it may be necessary to decode the document format and its character
encodings. 
The second step is to tokenize the text, which means splitting up
the text into the smallest meaningful entities, which in this context are
words. Depending on how advanced the tokenizer is, it should be able to handle
abbreviations, such as ``aren't'' or ``kg''. The tokenizer should also remove
stopwords, which are words that do not contain any meaningful information,
and should therefore not be indexed. Examples of this are words such as ``the''
and ``a''. As these words do not refer to anything in particular, a query should
not match indexed pages based on them \citep[Ch.
2]{manning2008introduction}.

\section{Indexing}
Indexing is used to speed up the information retrieval process, as it is faster
to search an index, than to scan through every single page every time a query is
performed.\\

There are different types of indexes which depends on the way that the tokens
and documents refer to each other. The two main types are the forward index and
inverted index.

\begin{minipage}{.40\textwidth}
  \centering
  \begin{table}[H]
	\centering
    \begin{tabular}{|l|l|}
\hline
Doc1 & fresh, tomato, soup \\ \hline
Doc2 & fresh, potato, soup \\ \hline
Doc3 & fresh, tomato, sauce \\ \hline
	\end{tabular}
	\caption{A forward index}
	\label{fIndex}
  \end{table}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \centering
  \begin{table}[H]
	\centering
    \begin{tabular}{|l|l|}
\hline
fresh & Doc1, Doc2, Doc3 \\ \hline
tomato & Doc1, Doc3 \\ \hline
soup & Doc1, Doc2 \\ \hline
sauce & Doc3 \\ \hline
	\end{tabular}
	\caption{A simple inverted index}
	\label{iIndex}
  \end{table}

\end{minipage}

In the forward index the pages themselves are indexed with a reference to the
tokens they contain, see \autoref{fIndex}. An inverted index is made up of
tokens referencing the documents which contains them, this is useful for
querying for specific tokens \citep{Index3}, see \autoref{iIndex}. More advanced
indexes can be expanded to include a ranking of the tokens in order to better
evaluate a documents information value. This ranking is usually determined by
how often the token appears in the documents and how many times it appears in a
specific document.