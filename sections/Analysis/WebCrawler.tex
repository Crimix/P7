\section{Web Crawler}\label{subsec:crawler}
A crawler, which is also known as a spider, systematically crawls the internet
and gather information by reading and analysing web-page's content. Web-pages
usually connect to each other by hyperlinks, which a crawler can use to move
between pages. This recursive approach to accessing web-pages allows the crawler
to access large parts of the internet systematically, and gather data along its
journey \citep[Ch20.2, P.444-445]{manning2008introduction}.

\subsection{Crawler design} 
There are several things to consider during a crawlers execution. A crawler must
be able to avoid getting stuck fetching an infinite number of pages from the
same domain, as this is only acceptable if you want to crawl that specific site.
The crawler needs to be ''polite'' about how frequent it makes requests to a
server, as too many to the same server can crash it, or cause the server to
consider the crawler an attacker and block it. The crawler should respect what
it is, and is not, permitted to access from a given domain. Additionally, the
crawler should also be able to re-fetch data from older sites, to see if they
have changed. Furthermore, it should favour fetching pages that are more
likely to have quality content \citep[Ch.20.1]{manning2008introduction}.\nl

A crawler's architecture is made to be modular, as seen in \autoref{BasicWC}.
The 'Frontier' is a list of elements, usually URLs, not yet crawled. The 'Fetch'
module is designed to get the firstelement from the 'Frontier' and request
access to the corresponding site. The site content is passed to the 'Parse'
module, which is further described in \autoref{sec:parsing}. The site content
gets verified to find if the site has been crawled before, it could be that the
side had minor updates or called itsef recursively causing the crawler to loop.
URLs get filtered according to the domains ``/robots.txt'', which describes what
each crawler is allowed to access. Finally, the duplicated URLs get eliminated,
and the new URLs added to the frontier.\nl

%Vejleder vil gerne have fikset overfull boxes men det er på den måde i bogen.
\figx[1.2]{BasicWC}{The basic crawler architecture \citep[p.
446]{manning2008introduction}.}
%Klaus: Some more sentances of explanation. It is always good to give the reader
% some basic understanding of what is going on in the caption, even if it
% doubles a bit with normal text

% Flesh out more
A more advanced version of a crawler involves managing multiple
distributed-crawlers. These crawlers need to work together to cover websites
faster and avoid crawling the same sites multiple times; it is also critical for
them to avoid spamming the same servers with requests.



\subsection{Parsing} \label{sec:parsing}
Parsing is a method used to extract information from a document, and index it
for later use. In this case, a document is the raw-content retrieved from a request,
including text, HTML code and -tags.
The first part of parsing a document consists of removing the unnecessary
information, such as HTML tags, while retaining the important information, like
links and the body text. Depending on the document, it may be necessary to
decode the document format and its character encodings.
The second step is to tokenize the text, which means splitting up the text into
the smallest meaningful entities, which in this context are words. Depending on
how advanced the tokenizer is, it should be able to handle abbreviations, such
as ``aren't'' or ``kg.''. The tokenizer should also remove stopwords, which are
words that do not contain useful information, and should therefore not
be indexed. Examples of this are words such as ``the'' and ``a''. As these words
do not refer to anything in particular, a query should not match indexed pages
based on them.

\subsection{Indexing}
Indexing is used to speed up the information retrieval process, as it is faster
to search an index than to scan through every single page every time a query is
performed.

There are different types of indexes, the best fit depends on the way
that the tokens and documents refer to each other. The two main types are the
forward index and inverted index, examples of such can be seen below.

\begin{minipage}{.40\textwidth}
  \centering
  \begin{table}[H]
	\centering
    \begin{tabular}{|l|l|}
\hline
Doc1 & fresh, tomato, soup \\ \hline
Doc2 & fresh, potato, soup \\ \hline
Doc3 & fresh, tomato, sauce \\ \hline
	\end{tabular}
	\caption{A forward index.}
	\label{fIndex}
  \end{table}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \centering
  \begin{table}[H]
	\centering
    \begin{tabular}{|l|l|}
\hline
fresh & Doc1, Doc2, Doc3 \\ \hline
tomato & Doc1, Doc3 \\ \hline
soup & Doc1, Doc2 \\ \hline
sauce & Doc3 \\ \hline
	\end{tabular}
	\caption{A simple inverted index.}
	\label{iIndex}
  \end{table}
\end{minipage}\nl

In a forward-index the pages themselves are indexed with a reference to the
tokens they contain, see \autoref{fIndex}. An inverted-index is made up of
tokens referencing the documents containing them, this is useful when
querying for specific tokens \citep{Index3}, see \autoref{iIndex}. More advanced
indexes can be expanded to include ranking of documents and tokens to better
evaluate a documents information value \citep[P. 109]{manning2008introduction}.

