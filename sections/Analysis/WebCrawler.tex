A crawler, which is also known as a spider, systematically crawls the internet
and gathers information by reading and analyzing the content of a web-pages.
Web-pages are usually connected to eachother by using hyperlinks, which the
crawler can use to move between pages. This recursive approach to accessing
web-pages allows to crawler to systematically access large parts of the
internet, and gather data along the way.

% General webcrawler structure
\subsection{Crawler design}%Fluffy start p√• section During execution, there are
several things for a crawler to consider.
The crawler must be able to avoid getting stuck fetching an infinite number of
pages from the same domain, as this is only acceptable if you want to crawl that
specific site.
The crawler needs to be ''polite'' about how frequently it makes request to a
server, as too many requests to the same server can crash it, or cause the
server to consider the crawler an attacker and block it.
The crawler should respect what it is, and is not, permitted to access from a
given domain. Additionally, the crawler should also be able to re-fetch data
from older sites, in order to see if they have been updated. Aditionally, it
should favour fetching pages that are more likely to have quality content \citep[Ch.
20.1]{manning2008introduction}.\nl
 
Generally, a crawler's architecture is made with a modular, which
can be seen in \autoref{BasicWC}. 
The 'Fetch' module is designed to take a URL from the
frontier and request access to the corresponding site. 
The site content is passed to the 'Parse' module, see \autoref{sec:parsing}.
The content is verified to see if it has been crawled before. The URLs are then
filtered according to the domains ``/robots.txt'', which describes what each
crawler is allowed to access. Finally, the duplicate URLs are eliminated, and
the new URLs are added to the frontier.\nl

\fix{}{Terminology like ``frontier'' needs to be explained before or during
this section}


\figx[1.2]{BasicWC}{The basic crawler architecture \citep[p.
446]{manning2008introduction}.}

% Flesh out more
A more advanced version of a crawler involves managing multiple distributed
crawlers, such that they are able to work together efficiently and not crawl
the same sites multiple times.

% Should it be called parsing instead?
\subsection{Parsing} \label{sec:parsing}
Parsing is a method used to extract information from a document and
index it for later use. The first part of parsing a document consists of
removing the unnecessary information, such as HTML tags, while retaining the
important information, such as links and the body text. Depending on the
document, it may be necessary to decode the document format and its character
encodings. 
The second step is to tokenize the text, which means splitting up
the text into the smallest meaningful entities, which in this context are
words. Depending on how advanced the tokenizer is, it should be able to handle
abbreviations, such as ``aren't'' or ``kg''. The tokenizer should also remove
stopwords, which are words that do not contain any meaningful information,
and should therefore not be indexed. Examples of this are words such as ``the''
and ``a''. As these words do not refer to anything in particular, a query should
not match indexed pages based on them \citep[Ch.
2]{manning2008introduction}.

\subsection{Indexing}
Indexing is used to speed up the information retrieval process, as it is faster
to search an index, than to scan through every single page every time a query is
performed.

There are different types of indexes which depends on the way that the tokens
and documents refer to each other. The two main types are the forward index and
inverted index.

\begin{minipage}{.40\textwidth}
  \centering
  \begin{table}[H]
	\centering
    \begin{tabular}{|l|l|}
\hline
Doc1 & fresh, tomato, soup \\ \hline
Doc2 & fresh, potato, soup \\ \hline
Doc3 & fresh, tomato, sauce \\ \hline
	\end{tabular}
	\label{fIndex}
	\caption{A forward index}
  \end{table}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \centering
  \begin{table}[H]
	\centering
    \begin{tabular}{|l|l|}
\hline
fresh & Doc1, Doc2, Doc3 \\ \hline
tomato & Doc1, Doc3 \\ \hline
soup & Doc1, Doc2 \\ \hline
sauce & Doc3 \\ \hline
	\end{tabular}
	\label{iIndex}
	\caption{A simple inverted index}
  \end{table}

\end{minipage}

In the forward index the pages themselves are indexed with a reference to the
tokens they contain, see \autoref{fIndex}. An inverted index is made up of
tokens referencing the documents which contains them, this is useful for
querying for specific tokens \citep{Index3}, see \autoref{iIndex}. More advanced
indexes can be expanded to include a ranking of the tokens in order to better
evaluate a documents information value. This ranking is usually determined by
how often the token appears in the documents and how many times it appears in a
specific document.