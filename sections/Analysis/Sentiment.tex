The large amount of data which can be gathered from social media will need to
analysed to be of actual use. One of these way that seems useful is sentiment
analysis. Sentiment analysis is used to determine the opinions of people
regarding specific topics, such as movies or products. This has become
especially useful with the increasing usage of social media, as people
increasingly discuss and post their opinions online \citep[Overview
2]{Sentiment}. Sentiment analysis is done by gathering opinions and extracting
tokens which provide clues to the opinion. The tokens can then be used with a
classification model to predict the sentiment.\nl

In \autoref{sec:FeatEx} we will look at how text is transformed into useful
features, and what options are available to increase the quality of the
features. In \autoref{sec:Class} we will look at how the features can be used in
models to make predictions. We will also describe ideas behind the 2 models
which are used in the program. Finally in \autoref{sec:DAConc} we will conclude
on the analysis.

\section{Feature Extraction}\label{sec:FeatEx}
Feature extraction is used after the tokenization, in order to choose which
words best determine the sentiment. A lot of the sentiment can be lost in the
feature extractor, as the opinions are much more apparent in text than as a
series of tokens. There are several ways which sentiment can be expressed in
text, such as\citep[Overview.3-4]{Sentiment}:

\begin{itemize}
  \item Capitalization 
  \item Lengthening
  \item Punctuation
  \item Stopwords
  \item Negation
\end{itemize}

For instance, there is a big difference between person 1 saying ``I love this
movie'' and person 2 saying ``I LOVE this movie!!!!''. In this example both the
capitalization of ``love'' and the exclamation mark help empathize that person
2 seems to have a stronger positive sentiment towards the movie.\nl

An example of lengthening is the difference in sentiment between ``huuuungry'',
``huuungry'' and ``hungry". The differences in sentiment between the first two
variations of ``hungry" are probably minimal, but there is a clear difference
between those and the last version. These features can be compressed by looking
for tokens with more than two of the same letters repeated and removing the
repeated letters. This will make the loss of sentiment minimal, while limiting
the varitions of the same word.\nl

It is useful to remove stopwords, which are words that do not contain useful
information. As they can appear in every type of sentence, they can make
classification more difficult, though it also mainly depends on the model,
see \autoref{subsub:Models}.\nl

Negation also needs to be considered, as the sentiment can be completely changed
with a word ``not''. Since sentiment is extracted from tokens, it is difficult
to judge how each tokens relate to others when seen individually. A simple way
to handle negation is to prefix tokens with a tag such as ``NEG\_'' after a
word like ``not'' and ``aren't''.\nl

An example of what a feature extraction can look like can be seen on table
\autoref{tab:feature}.

\begin{table}[H]
\centering
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
Text & Features \\ \hline
I LOVED the movie but I am sooooo hungry! & 
``I'' ``LOVED'' ``movie'' ``I'' ``soo'' ``hungry'' ``!''
\\ \hline 
I don't like this song, why do they keep playing it? &
``I'' ``dont'' ``like'' ``NEG\_song'' ``NEG\_why'' ``NEG\_keep'' ``NEG\_playing''
``?'' \\ \hline
\end{tabular}
\caption{An example of transforming text into features}
\label{tab:feature}
\end{table}

There are a couple of other methods that can affect how much sentiment is
retained in the text, such as stemming and using n-grams.

\subsection{Stemming}\label{subsec:Stem}
Stemming can be used when performing feature extraction. It is used to collapse
a word into its base, for instance ``finding'' can be stemmed to ``find''.
Stemming is both useful and destructive. It allows for reduction in the number
of terms as they can be collapsed. It is also destructive as sentiment clues
can be lost easily. A lot of the meaning of a word can be tied to the ending of
the word, such as ``captivating'' and ``captive''. By using the Porter Stemmer
Algorithm both words will collapse to ``captiv'' which removes what
differentiate them. There are other Stemmer algorithms but each provide similar
problems. Stemming can also be useful as it can reduce the vocabulary, instead
of remembering both ``captivating'' and ``captive'', it now only needs to
remember ``captiv''. This can be useful for small datasets where each token is
important, as it can reduce the vocabulary size and sharpen the result
\citep[Ch 3.b]{Sentiment}.

\subsection{N-Grams}
N-grams are sequence of 'n' items and determines how much context is stored
from a piece of text. An example of the 3 first n-grams can be seen on table
\autoref{tab:ngram}. 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
Text & ``I love horror movies'' \\ \hline
Unigrams (1) &
``I'' ``love'' ``horror'' ``movies''
\\ \hline 
Bigram (2) &
``I love'' ``love horror'' ``horror movies''
\\ \hline
Trigram (3) &
``I love horror'' ``love horror movies''
\\ \hline
\end{tabular}
\caption{An example of different N-Grams}
\label{tab:ngram}
\end{table}

The idea is that as bigger n-grams are used, more context is stored in the
token. It becomes less likely to encounter a given token, which should
improve accuracy if there is a large training sample. With smaller training
samples it becomes necessary to use unigrams as there are not enough
token occurances to justify using bigger n-grams.

\subsection{Feature Vectors}\label{sub:FeatureVector}
A feature vector is used to denote which features are present in a piece of
text. The vector gets created by mapping each feature to a unique integer, this
is done while training the model and is called a Bag-Of-Words (not to be
confused with the Bag-Of-Words classification model, described in
\autoref{subsub:Models}). With this Tokens-To-Integers the feature vector can
now be created as a series, often array, of 1's or 0's, which denotes whether
the given feature is present in the vector. The bigger the Bag-Of-Words is, the
larger every feature vector is. For example if ``\textit{love}'' is mapped to
\textbf{3} and ``\textit{movie}'' is mapped to \textbf{205}, then a feature
vector for ``\textit{I love this movie}'' could be: [0,0,0,1,0,\ldots,0,1,0].



\section{Classification}\label{sec:Class}
The final part of sentiment analysis is the classification task. This task
consists of predicting the sentiment based on the features extracted earlier.
The accuracy of the predictions are based on labeled data that was provided as
training data, as well as the model that is used. 

\subsection{Training}\label{subsec:Train}
The training data is an important part of every model. There are three
different training methods, each suitable for a different task.

\begin{itemize}
  \item \textbf{Supervised learning} is when the training data is paired as an
  input-output pair, for instance: ``This is a great movie'' can be paired with
  ``positive''. This allows the model to verify its prediction whether it
  guesses correct or not \citep[Ch. 7.0]{MIBook}. This is useful as it provides
  an easy way to validate the accuracy of the model. The main problem is that
  most data is not labeled, which means that it has to be labeled to be useful.
  \item \textbf{Reinforcement learning} is similar to supervised learning with
  the key difference being that the model is only given a ``reward'' when it
  performs the best action. This type of learning method is useful when
  training a robotic agent \citep{Reinforcement}.
  \item \textbf{Unsupervised learning} is different as the training data no
  longer includes a result, and as such, it can no longer evaluate the accuracy
  of the prediction. Instead, it is allows for clustering data and aims to
  minimize prediction errors \citep[Ch. 11.1]{MIBook}.
\end{itemize}

The model, which is most used for sentiment analysis, is the supervised learning
approach, as it is the most useful for classification tasks.

\subsection{Models}\label{subsub:Models}

There are a several different models which can be used for sentiment analysis. 
Different models provide different levels of accuracy, as seen in
\autoref{sentiment}, which shows a several of different models on the same IMDB
dataset with three different outputs. While we do not go in depth about the
models that are available, we will describe the models that are of
immediate interest to us. A cursory description of different models can be
found in the citation \citep{Classification}.

\figx[1.0]{sentiment}{Accuracy of different sentiment analysis models on the
IMDB dataset \citep{Classification}}

Our main focus will be on the Bag-Of-Words and Naive Bayes models, as both
straight forwards methods of determining sentiment, which should help with
providing proof of concept.

\subsubsection{Bag-Of-Words}
The Bag-Of-Words model determines the sentiment of a text by
counting the occurrences of words with either a negative of positive sentiment.
Words can have different sentiment scores, that is, one word may be percieved
as conveying a more intense sentiment than others. Each word and its
corresponding sentiment score is stored in a sentiment lexicon, these lexicons
can be created by or found freely online \citep{BagOfWords}.

\fix{add a small table to illustrate}{}


\subsubsection{Naive Bayes} 
The Naive Bayes classifier uses Bayes' rule and the assumption of independence,
which simply assumes that the features are independent of each other
\citep[Proposition 6.5]{MIBook}, to determine the sentiment. Bayes' rule, see
\autoref{e:Bayes}, describes how prior knowledge can relate to and affect an
event \citep[P.229]{MIBook}. 

\begin{equation}\label{e:Bayes}
P(X|Y) = \cfrac{P(Y|X) \cdot P(X)}{P(Y)}
\end{equation}


It trains using supervised learning, where each text object
is labeled with a class. The text features are extracted as tokens.
The probability of each token belonging to each of the labels is then
calculated by counting how often it occurs with that class. This leads to a
table such as \autoref{tab:NB}.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Word & Positive & Negative & Total 	\\ \hline
this & 45 & 55 & 100				\\ \hline
movie & 115 & 85 & 200				\\ \hline
rule & 90 & 10 & 100				\\ \hline
suck & 5 & 95 & 100					\\ \hline
Total & 255 & 245 & 500				\\ \hline
\end{tabular}
\caption{A table where each token is classified with a label}
\label{tab:NB}
\end{table}

In order to predict which class, \textit{C}, a given piece of text belongs to it
needs to be transformed to a feature vector, \textit{W}, where each feature is
denoted as, $w_{i}$. By using \autoref{e:Score} we can predict the class
\citep[Ch.2.1]{Bayes}.
\begin{equation}\label{e:Score}
score(W,C) = P(C) \cdot \displaystyle\prod_{i=1}^{n}P(w_{i}|C)
\end{equation}

The idea is calculate the probability of the feature vector belonging to each of
the classes. The class probabilities $P(C)$ are determined how frequently the classes have
been observed to occur, therefor:
\begin{center}
$P(C_{Pos}) = \cfrac{255}{500} = 0.51 $ and $P(C_{neg}) = \cfrac{245}{500} =
0.49 $
\end{center}
The probabilities of each of the features are likewise found by how often the
features occur. For instance, if we want to determine the class of ``this
movie rule'', with the probabilities from \autoref{tab:NB}, we have get the
following feature vector [1,1,1,0]. We can then calculate the score of
the feature vector belonging to each of the classes:

\begin{equation}\label{e:Pos}
score(W, C_{Pos}) = \cfrac{255}{500} \cdot \cfrac{45}{100} \cdot
\cfrac{115}{200} \cdot \cfrac{90}{100} = 0.238
\end{equation}
\begin{equation}\label{e:Neg}
score(W, C_{Neg}) = \cfrac{245}{500} \cdot \cfrac{55}{100} \cdot \cfrac{85}{200}
\cdot \cfrac{10}{100} = 0.019
\end{equation}

Then by taking the highest score we the can predict the class the text belongs
to, in this case it would be positive.

\section{Conclusion}\label{sec:DAConc}
After considering how sentiment analysis is used to analyse data, we can
conclude that it has distinct uses for classifying large amounts of user data.
It is especially useful for analysing the sentiment around american politics as
there are only two main parties with opposing views, which means that not only
is there few classes, they should also be very distinct. By determining the
sentiment it becomes possible to see if a user is inside a political bubble,
where the user only encounters the same sentiment.



%https://www.cs.cornell.edu/home/llee/papers/sentiment.pdf