\section{Sentiment Analysis}\label{SentiAnal}
Sentiment analysis is used to determine the opinions of people regarding
specific topics, such as movies or products. This has become especially useful
with the increasing usage of social media, as people increasingly discuss and
post their opinions online. Sentiment analysis is done by
gathering opinions and extracting tokens which provide clues to the opinion. The tokens can then be
used with a classification model to predict the sentiment. 

\subsection{Feature extraction} 
Feature extraction is used after the tokenization, in order to choose which
words best determine the sentiment. A lot of the sentiment can be lost in the
feature extractor, as the opinions are much more apparent in text than as a
series of tokens. There are several ways which sentiment can be expressed in
text, such as\citep[Overview.3-4]{Sentiment}:

\begin{itemize}
  \item Capitalization 
  \item Lengthening
  \item Punctuation
  \item Stopwords
  \item Negation
\end{itemize}

For instance, there is a big difference between person 1 saying ``I love this
movie'' and person 2 saying ``I LOVE this movie!!!!''. In this example both the
capitalization of ``love'' and the exclamation mark help empathize that person
2 seems to have a stronger positive sentiment towards the movie.\\


An example of lengthening is the difference in sentiment between ``huuuungry'',
``huuungry'' and ``hungry". The differences in sentiment between the first two
variations of ``hungry" are probably minimal, but there is a clear difference
between those and the last version. These features can be compressed by looking
for tokens with more than two of the same letters repeated and removing the
repeated letters. This will make the loss of sentiment minimal, while limiting
the varitions of the same word.\nl

It is useful to remove stopwords, which are words that do not contain useful
information. As they can appear in every type of sentence, they can make
classification more difficult, though it also mainly depends on the model,
see \autoref{subsub:Models}.\nl

Negation also needs to be considered, as the sentiment can be completely changed
with a word ``not''. Since sentiment is extracted from tokens, it is difficult
to judge how each tokens relate to others when seen individually. A simple way
to handle negation is to prefix tokens with a tag such as ``NEG\_'' after a
word like ``not'' and ``aren't''.\nl

An example of what a feature extraction can look like can be seen on table
\autoref{tab:feature}.

\begin{table}[H]
\centering
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
Text & Features \\ \hline
I LOVED the movie but I am sooooo hungry! & 
``I'' ``LOVED'' ``movie'' ``I'' ``soo'' ``hungry'' ``!''
\\ \hline 
I don't like this song, why do they keep playing it? &
``I'' ``dont'' ``like'' ``NEG\_song'' ``NEG\_why'' ``NEG\_keep'' ``NEG\_playing''
``?'' \\ \hline
\end{tabular}
\caption{An example of transforming text into features}
\label{tab:feature}
\end{table}

There are a couple of other methods that can affect how much sentiment is
retained in the text, such as stemming and using n-grams.

\subsubsection{Stemming}
Stemming can be used when performing feature extraction. It is used to collapse
a word into its base, for instance ``finding'' can be stemmed to ``find''.
Stemming is both useful and destructive. It allows for reduction in the number
of terms as they can be collapsed. It is also destructive as sentiment clues
can be lost easily. A lot of the meaning of a word can be tied to the ending of
the word, such as ``captivating'' and ``captive''. By using the Porter Stemmer
Algorithm both words will collapse to ``captiv'' which removes what
differentiate them. There are other Stemmer algorithms but each provide similar
problems. Stemming can also be useful as it can reduce the vocabulary, instead
of remembering both ``captivating'' and ``captive'', it now only needs to
remember ``captiv''. This can be useful for small datasets where each token is
important, as it can reduce the vocabulary size and sharpen the result
\citep[Ch 3.b]{Sentiment}.

\subsubsection{N-Grams}
N-grams are sequence of 'n' items and determines how much context is stored
from a piece of text. An example of the 3 first n-grams can be seen on table
\autoref{tab:ngram}. 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
Text & ``I love horror movies'' \\ \hline
Unigrams (1) &
``I'' ``love'' ``horror'' ``movies''
\\ \hline 
Bigram (2) &
``I love'' ``love horror'' ``horror movies''
\\ \hline
Trigram (3) &
``I love horror'' ``love horror movies''
\\ \hline
\end{tabular}
\caption{An example of different N-Grams}
\label{tab:ngram}
\end{table}

The idea is that as bigger n-grams are used, more context is stored in the
token. It becomes less likely to encounter a given token, which should
improve accuracy if there is a large training sample. With smaller training
samples it becomes necessary to use unigrams as there are not enough
token occurances to justify using bigger n-grams.

\subsubsection{Feature Vectors}
A feature vector is used to denote which features are present in a piece of
text. The vector gets created by mapping each feature to a unique integer, this
is done while training the model and is called a Bag-Of-Words (not to be
confused with the Bag-Of-Words model, described in \autoref{subsub:Models}).
With this Tokens-To-Integers the feature vector can now be created as a series,
often array, of 1's or 0's, which denotes whether the given feature is present
in the vector. The bigger the Bag-Of-Words is, the larger every feature vector
is. For example if ``\textit{love}'' is mapped to \textbf{3} and
``\textit{movie}'' is mapped to \textbf{205}, then a feature vector for
``\textit{I love this movie}'' could be: [0,0,0,1,0,\ldots,0,1,0].



\subsection{Classification}
The final part of sentiment analysis is the classification task. This task
consists of predicting the sentiment based on the features extracted earlier.
The accuracy of the predictions are based on labeled data that was provided as
training data, as well as the model that is used. 

\subsubsection{Training}
The training data is an important part of every model. There are three
different training methods, each suitable for a different task.

\begin{itemize}
  \item \textbf{Supervised learning} is when the training data is paired as an
  input-output pair, for instance: ``This is a great movie'' can be paired with
  ``positive''. This allows the model to verify its prediction whether it
  guesses correct or not \citep[Ch. 7.0]{MIBook}. This is useful as it provides
  an easy way to validate the accuracy of the model. The main problem is that
  most data is not labeled, which means that it has to be labeled to be useful.
  \item \textbf{Reinforcement learning} is similar to supervised learning with
  the key difference being that the model is only given a ``reward'' when it
  performs the best action. This type of learning method is useful when
  training a robotic agent \citep{Reinforcement}.
  \item \textbf{Unsupervised learning} is different as the training data no
  longer includes a result, and as such, it can no longer evaluate the accuracy
  of the prediction. Instead, it is allows for clustering data and aims to
  minimize prediction errors \citep[Ch. 11.1]{MIBook}.
\end{itemize}

The model, which is most used for sentiment analysis, is the supervised learning
approach, as it is the most useful for classification tasks.

\subsubsection{Models}\label{subsub:Models}

There are a several different models which can be used for sentiment analysis. 
Different models provide different levels of accuracy, as seen in
\autoref{sentiment}, which shows a several of different models on the same IMDB
dataset with three different outputs. While we do not go in depth about the
models that are available, we will describe the models that are of
immediate interest to us. A cursory description of different models can be
found in the citation \citep{Classification}.

\figx[1.0]{sentiment}{Accuracy of different sentiment analysis models on the
IMDB dataset \citep{Classification}}

Our main focus will be on the Bag-Of-Words and Naive Bayes models, as both
straight forwards methods of determining sentiment.

% \fix{The models that are of interest to use are the Bag-of-Words model, the
% Naive Bayes Classifier, the Support Vector Machine and the Neural Network.}{add
% something about SVM and NN?}

\paragraph{The Bag-Of-Words} model determines the sentiment of a text by
counting the occurrences of words with either a negative of positive sentiment.
Words can have different sentiment scores, that is, one word may be percieved
as conveying a more intense sentiment than others. Each word and its
correspondig sentiment score is stored in a sentiment lexicon, these lexicons
cna be created by or found freely online\citep{BagOfWords}.

\paragraph{The Naive Bayes} classifier determines the sentiment of text using
probability. It trains using supervised learning, where each text object
is labeled with a class. The text features are extracted as tokens.
The probability of each token belonging to each of the labels is then
calculated by counting how often it occurs with that class. This leads to a
table such as \autoref{tab:NB}.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Word & Positive & Negative & Total 	\\ \hline
this & 45 & 55 & 100				\\ \hline
movie & 115 & 85 & 200				\\ \hline
rule & 90 & 10 & 100				\\ \hline
suck & 5 & 95 & 100					\\ \hline
Total & 255 & 245 & 500				\\ \hline
\end{tabular}
\caption{A table where each token is classified with a label}
\label{tab:NB}
\end{table}

%, \textit{w_{i}},
%, \textit{C}



A class, \textit{C}, 



In order to predict which class, \textit{C}, a given piece of text belongs to it
needs to be transformed to a feature vector. A feature 

 we transform it , we use

With the
Bayes Theorem which

$P(C|X) = \cfrac{P(X|C)*P(C)}{P(X)}$

\citep[p.229]{MIBook}
\citep[Ch.2.1]{Bayes}
The probabilities are\ldots

The class probabilities, positive and negative, are then determined by:
\begin{center}
$P(C_{Pos}) = \cfrac{255}{500} = 0.51 $ and $P(C_{neg}) = \cfrac{245}{500} =
0.49 $
\end{center}

We then determine the sentiment by using:



If we want to determine the sentiment of ``this movie rule''  
$ P(C_{Pos}) = \cfrac{255}{500} \cdot \cfrac{45}{100} \cdot \cfrac{115}{200}
\cdot \cfrac{90}{100} = 0.238$
$ P(C_{Neg}) = \cfrac{245}{500} \cdot \cfrac{55}{100} \cdot \cfrac{85}{200}
\cdot \cfrac{10}{100} = 0.019$

 





Conditional independence does not hold.












%https://www.cs.cornell.edu/home/llee/papers/sentiment.pdf