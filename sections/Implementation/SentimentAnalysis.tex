\section{Sentiment Analysis}\label{SentiAnal}
Sentiment analysis is used to determine the opinions of people regarding
specific topics, such as movies or products. This has become especially useful
with the increasing usage of social media, as people increasingly discuss and
post their opinions online. Sentiment analysis is done by
gathering opinions and extracting tokens which provide clues to the opinion. The tokens can then be
used with a classification model to predict the sentiment. 

\subsection{Feature extraction} 
Feature extraction is used after the tokenization, in order to choose which
words best determine the sentiment. A lot of the sentiment can be lost in the
feature extractor, as the opinions are much more apparent in text than as a
series of tokens. There are several ways which sentiment can be expressed in
text, such as\citep[Overview.3-4]{Sentiment}:

\begin{itemize}
  \item Capitalization 
  \item Lengthening
  \item Punctuation
  \item Stopwords
  \item Negation
\end{itemize}

For instance, there is a big difference between person 1 saying ``I love this
movie'' and person 2 saying ``I LOVE this movie!!!!''. In this example both the
capitalization of ``love'' and the exclamation mark help empathize that person
2 seems to have a stronger positive sentiment towards the movie.\\


An example of lengthening is the difference in sentiment between ``huuuungry'',
``huuungry'' and ``hungry". The differences in sentiment between the first two
variations of ``hungry" are probably minimal, but there is a clear difference
between those and the last version. These features can be compressed by looking
for tokens with more than two of the same letters repeated and removing the
repeated letters. This will make the loss of sentiment minimal, while limiting
the varitions of the same word.\nl

It is useful to remove stopwords, which are words that do not contain useful
information. As they can appear in every type of sentence, they can make
classification more difficult, though it also mainly depends on the model,
see \autoref{subsub:Models}.\nl

Negation also needs to be considered, as the sentiment can be completely changed
with a word ``not''. Since sentiment is extracted from tokens, it is difficult
to judge how each tokens relate to others when seen individually. A simple way
to handle negation is to prefix tokens with a tag such as ``NEG\_'' after a
word like ``not'' and ``aren't''.\nl

An example of what a feature extraction can look like can be seen on table
\autoref{tab:feature}.

\begin{table}[H]
\centering
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
Text & Features \\ \hline
I LOVED the movie but I am sooooo hungry! & 
``I'' ``LOVED'' ``movie'' ``I'' ``soo'' ``hungry'' ``!''
\\ \hline 
I don't like this song, why do they keep playing it? &
``I'' ``dont'' ``like'' ``NEG\_song'' ``NEG\_why'' ``NEG\_keep'' ``NEG\_playing''
``?'' \\ \hline
\end{tabular}
\caption{An example of transforming text into features}
\label{tab:feature}
\end{table}

There are a couple of other methods that can affect how much sentiment is
retained in the text, such as stemming and using n-grams.

\subsubsection{Stemming}
Stemming can be used when performing feature extraction. It is used to collapse
a word into its base, for instance ``finding'' can be stemmed to ``find''.
Stemming is both useful and destructive. It allows for reduction in the number
of terms as they can be collapsed. It is also destructive as sentiment clues
can be lost easily. A lot of the meaning of a word can be tied to the ending of
the word, such as ``captivating'' and ``captive''. By using the Porter Stemmer
Algorithm both words will collapse to ``captiv'' which removes what
differentiate them. There are other Stemmer algorithms but each provide similar
problems. Stemming can also be useful as it can reduce the vocabulary, instead
of remembering both ``captivating'' and ``captive'', it now only needs to
remember ``captiv''. This can be useful for small datasets where each token is
important, as it can reduce the vocabulary size and sharpen the result
\citep[Ch 3.b]{Sentiment}.

\subsubsection{N-Grams}
N-grams are sequence of 'n' items and determines how much context is stored
from a piece of text. An example of the 3 first n-grams can be seen on table
\autoref{tab:ngram}. 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
Text & ``I love horror movies'' \\ \hline
Unigrams (1) &
``I'' ``love'' ``horror'' ``movies''
\\ \hline 
Bigram (2) &
``I love'' ``love horror'' ``horror movies''
\\ \hline
Trigram (3) &
``I love horror'' ``love horror movies''
\\ \hline
\end{tabular}
\caption{An example of different N-Grams}
\label{tab:ngram}
\end{table}

The idea is that as bigger n-grams are used, more context is stored in the
token. It becomes less likely to encounter a given token, which should
improve accuracy if there is a large training sample. With smaller training
samples it becomes necessary to use unigrams as there are not enough
token occurances to justify using bigger n-grams.


\subsection{Classification}
The final part of sentiment analysis is the classification task. This task
consists of predicting the sentiment based on the features extracted earlier.
The accuracy of the predictions are based on labeled data that was provided as
training data, as well as the model that is used. 

\subsubsection{Training}
The training data is an important part of every model. There are three different
training methods, each suitable for a different task.

\begin{itemize}
  \item \textbf{Supervised learning} is when the training data is paired as an
  input-output pair, for instance: ``This is a great movie'' can be paired with
  ``positive''. This allows the model to verify its prediction whether it
  guesses correct or not \citep[Ch. 7.0]{MIBook}. This is useful as it provides
  an easy way to validate the accuracy of the model. The main problem is that
  most data is not labeled, which means that it has to be labeled to be useful.
  \item \textbf{Reinforcement learning} is similar to supervised learning with
  the key difference being that the model is only given a ``reward'' when it
  performs the best action. This type of learning method is useful when
  training a robotic agent \citep{Reinforcement}.
  \item \textbf{Unsupervised learning} is different as the training data no
  longer includes a result, and as such, it can no longer evaluate the accuracy
  of the prediction. Instead, it is allows for clustering data and aims to
  minimize prediction errors \citep[Ch. 11.1]{MIBook}.
\end{itemize}

The model, which is most used for sentiment analysis, is the supervised learning
approach, as it is the most useful for classification tasks.

\subsubsection{Models}\label{subsub:Models}

There are a lot of different models which can be used for sentiment analysis. 
Different models provide different levels of accuracy, as seen in
\autoref{sentiment}, which shows a several of different models on the same IMDB
dataset with three different outputs. While We do not go in depth about the
models that are available, we will describe the models that are of interest. A
cursory description of the different models can be found in the citation
\citep{Classification}.

\figx[1.0]{sentiment}{Accuracy of different sentiment analysis models on the
IMDB dataset \citep{Classification}}

\fix{The models that interest us are the Naive Bayes model, Support Vector Machines
and Neural Networks. }{Decide which models we need to describe\ldots}

\begin{itemize}
  \item \textbf{Bag of Words} works by\ldots
  \item \textbf{Naive Bayes Classifier} is a probabilistic model that tries to
  predict which class a given a number of features belong to. 
\end{itemize}






The different models, which are shown in the graph, are Naive Bayes using 

classification:
- Naive Bayes
- Support Vector Machines
- Neural Networks

